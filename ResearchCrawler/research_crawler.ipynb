{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import traceback\n",
    "from requests import HTTPError, ConnectionError\n",
    "from collections import defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = ['http://cs.mines.edu/CS-Faculty',\n",
    "            'http://science.iit.edu/computer-science/people/faculty',\n",
    "            'http://cms.bsu.edu/academics/collegesanddepartments/computerscience/facultyandstaff/faculty',\n",
    "            'http://www.memphis.edu/cs/people/index.php',\n",
    "            'http://www.smu.edu/Lyle/Departments/CSE/People/Faculty',\n",
    "            'https://www.csuohio.edu/engineering/eecs/faculty-staff',\n",
    "            'http://computerscience.engineering.unt.edu/content/faculty',\n",
    "            'https://www.odu.edu/compsci/research',\n",
    "            'http://www.cs.ucf.edu/people/index.php',\n",
    "            ]\n",
    "examples = ['https://inside.mines.edu/CS-Faculty-and-Staff/TracyCamp',\n",
    "            'http://science.iit.edu/people/faculty/eunice-santos',\n",
    "            'http://cms.bsu.edu/academics/collegesanddepartments/computerscience/facultyandstaff/faculty/buispaul',\n",
    "            'http://www.memphis.edu/cs/people/faculty_pages/william-baggett.php',\n",
    "            'http://www.smu.edu/Lyle/Departments/CSE/People/Faculty/ThorntonMitchell',\n",
    "            'https://www.csuohio.edu/engineering/charles-k-alexander-professor',\n",
    "            'http://www.cse.unt.edu/~rakl',\n",
    "            'https://www.odu.edu/directory/people/a/achernik',\n",
    "            'http://www.cs.ucf.edu/~bagci',\n",
    "            ]\n",
    "\n",
    "url_check = {'http://cs.mines.edu/CS-Faculty': 15,\n",
    "             'http://science.iit.edu/computer-science/people/faculty': 52,\n",
    "             'http://cms.bsu.edu/academics/collegesanddepartments/computerscience/facultyandstaff/faculty': 19,\n",
    "             'http://www.memphis.edu/cs/people/index.php': 16,\n",
    "             'http://www.smu.edu/Lyle/Departments/CSE/People/Faculty': 17,\n",
    "             'https://www.csuohio.edu/engineering/eecs/faculty-staff': 32,\n",
    "             'http://computerscience.engineering.unt.edu/content/faculty': 34,\n",
    "             'https://www.odu.edu/compsci/research': 15,\n",
    "             'http://www.cs.ucf.edu/people/index.php': 54,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contain_keys(href, keys, is_name=False):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if not keys:\n",
    "        return False\n",
    "    words = \"(%s)\" % '|'.join(e for e in keys)\n",
    "    if is_name and re.search('%s' % words, href, re.I):\n",
    "        return True\n",
    "    if re.search(r'\\b%s\\b' % words, href, re.I):\n",
    "        return True\n",
    "    if re.search(r'_?%s_?' % words, href, re.I):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def format_url(href, domain, index=''):\n",
    "\n",
    "    if href.startswith('http'):\n",
    "        full_url = href\n",
    "    elif href.startswith('//'):\n",
    "        full_url = 'http:' + href\n",
    "    elif href[0] == '/' and (len(href) == 1 or href[1] != '/'):\n",
    "        full_url = domain + href\n",
    "    elif href.find(domain) > -1:\n",
    "        full_url = 'http://' + href\n",
    "    else:\n",
    "        full_url = index + href if index[-1] == '/' else index + '/' + href\n",
    "    return full_url\n",
    "\n",
    "\n",
    "def get_and_store_page(page_url):\n",
    "\n",
    "    parts = page_url.split(\"/\")[2].split(\".\")\n",
    "    university_name = parts[-2]\n",
    "    dir_name = os.path.join(os.environ.get('OPENSHIFT_PYTHON_LOG_DIR', '.'), 'data', university_name)\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    file_name = re.sub(\"[?%=]\", \"\", dir_name+'/' + (page_url.split('/')[-1] if page_url[-1] != '/' else page_url.split('/')[-2]) + '.html')\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name) as fp:\n",
    "            html = fp.read()\n",
    "    else:\n",
    "        try:\n",
    "            r = requests.get(page_url)\n",
    "            html = r.content\n",
    "        except (ConnectionError, HTTPError), e:\n",
    "            html = \"Error at \", page_url\n",
    "\n",
    "        with open(file_name, 'w') as fp:\n",
    "            fp.write(html)\n",
    "    return html\n",
    "\n",
    "\n",
    "def onsocial(href):\n",
    "    href = href.lower()\n",
    "    for e in ['facebook', 'twitter', 'google', 'youtube', 'calendar']:\n",
    "        if e in href:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def same_tags(a, b):\n",
    "    if len(a) != len(b):\n",
    "        return False\n",
    "    for i in range(len(a)):\n",
    "        if a[i] != b[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResearchCrawler:\n",
    "    \"\"\"\n",
    "    从院系的Faculty目录中爬取有内容的教授信息\n",
    "    如研究兴趣、招生机会\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.url = \"\"\n",
    "        self.university_name = \"\"\n",
    "        self.domain = \"\"\n",
    "        self.example = \"\"\n",
    "        self.key_words = None\n",
    "        self.load_key()\n",
    "\n",
    "    def crawl_faculty_list(self, directory_url, example):\n",
    "        self.example = example\n",
    "        self.url = directory_url\n",
    "        self.university_name = re.search('(\\w+).edu', self.url).group(1)\n",
    "        self.domain = '/'.join(directory_url.split(\"/\")[:3])\n",
    "        self.key_words['stop_word'] += re.findall(\"(\\w+)\", directory_url)\n",
    "        \n",
    "        content, soup = self.open_page(directory_url)\n",
    "        anchors = self.find_all_anchor(soup, self.domain, directory_url)\n",
    "        # print directory_url, len(anchors)\n",
    "        index = self.find_example_index(anchors, example)\n",
    "        # print directory_url, len(anchors[index:]), \n",
    "\n",
    "        # 第一个教授主页的作用主要在这里——如果能再来一个更好\n",
    "        # 求共同祖先\n",
    "        count, faculty_list = self.find_faculty_list(anchors[index:], directory_url)\n",
    "        return count, faculty_list\n",
    "    \n",
    "    def crawl_from_directory(self, directory_url, example):\n",
    "        \"\"\"\n",
    "        从目录爬\n",
    "        \"\"\"\n",
    "\n",
    "        count, faculty_list = self.crawl_faculty_list(directory_url, example)\n",
    "        # assert count >= url_check[url]\n",
    "        result = []\n",
    "        for one in faculty_list:\n",
    "            result.append(self.dive_into_page(one))\n",
    "        return result\n",
    "\n",
    "    def load_key(self):\n",
    "        with open(os.path.join(os.path.dirname(__name__), 'key.json')) as fp:\n",
    "            self.key_words = json.loads(fp.read(), strict=False)\n",
    "        if self.key_words is None:\n",
    "            return 'Error'\n",
    "\n",
    "    def open_page(self, page_url):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            html = get_and_store_page(page_url)\n",
    "            if html.startswith(\"Error at \"):\n",
    "                return \"Error to load\", None\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            if soup.find(\"noframes\"):\n",
    "                for e in soup.find_all(\"frame\"):\n",
    "                    if contain_keys(e.get(\"src\"), self.key_words[\"div_pass\"]):\n",
    "                        continue\n",
    "                    page_url = page_url + e.get(\"src\") if page_url[-1] == '/' else page_url + '/' + e.get(\"src\") \n",
    "                    html = get_and_store_page(page_url)\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "            return html, soup\n",
    "        except Exception, e:\n",
    "            traceback.print_exc()\n",
    "            return \"Error to load\", None\n",
    "\n",
    "    def find_all_anchor(self, soup, domain, page_url):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        l = soup.find_all('a')\n",
    "        return l\n",
    "\n",
    "    def filter_list(self, e):\n",
    "        name = e.string\n",
    "        href = e.get('href')\n",
    "        base_faculty_filter = [e for e in self.example.split('/')\n",
    "                               if e and not e.startswith('http') and not e.endswith('.edu')]\n",
    "\n",
    "        if href and '~' in href:\n",
    "            return False\n",
    "        elif name is not None and contain_keys(name, self.key_words['site_flag']):\n",
    "            return False\n",
    "        elif href and href.startswith('mailto:'):\n",
    "            return True\n",
    "        elif not href or len(href) < 5 or base_faculty_filter[0] not in href:\n",
    "            return True\n",
    "        elif contain_keys(href, self.key_words['notprof']):\n",
    "            return True\n",
    "        elif not contain_keys(href, self.key_words['keys']):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def find_example_index(self, l, a):\n",
    "        for i in range(len(l)):\n",
    "            href = l[i].get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "            href = format_url(l[i].get(\"href\"), self.domain, a)\n",
    "        #   print page_url, a\n",
    "            if href == a:\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "    def filter_research_interests(self, alist):\n",
    "        \"\"\"\n",
    "        暂时只想到这么多条件\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for e in alist:\n",
    "            if e.parent.name == 'a':\n",
    "                continue\n",
    "            if contain_keys(e, self.key_words['notresearch']):\n",
    "                continue\n",
    "            result.append(e)\n",
    "        return result\n",
    "\n",
    "    def select_line_part(self, line):\n",
    "        pos = 0\n",
    "        for flag in self.key_words['interest_line_mode']:\n",
    "            new_pos = line.find(flag)    \n",
    "            if new_pos > pos:\n",
    "                pos = new_pos + len(flag)\n",
    "                break\n",
    "        return line[pos:]\n",
    "\n",
    "    def replace_words(self, line):\n",
    "        line = re.sub(\"\\s+\", \" \", line)\n",
    "        line = line.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        for x in self.key_words['replace']:\n",
    "            line = re.sub(r'\\b%s\\b' % x, ',', line)\n",
    "        return line\n",
    "\n",
    "    def get_open_position(self, soup, content):\n",
    "        open_position = False\n",
    "        open_term = \"\"\n",
    "        text = soup.get_text()\n",
    "        if contain_keys(text, self.key_words[\"open_position\"], True):\n",
    "            open_position = True\n",
    "        if contain_keys(text, self.key_words[\"open_term\"], True):\n",
    "            open_term = \"always\"\n",
    "        return open_position, open_term\n",
    "    \n",
    "    def extract_from_sibling(self, node, tags):\n",
    "        # print node.name, node.string\n",
    "        next_node = node.find_next_sibling()\n",
    "        # print next_node\n",
    "        if next_node and next_node.name == 'ul':\n",
    "            \n",
    "            for e in next_node.strings:\n",
    "                if e and e.strip():\n",
    "                    tags = self.extract_from_line(e.strip(), tags)\n",
    "            return tags\n",
    "        for n in node.find_next_siblings():\n",
    "            if n.name != next_node.name:\n",
    "                break\n",
    "            tags = self.extract_from_line(n.text, tags)\n",
    "        return tags\n",
    "\n",
    "    def extract_from_line(self, line, tags):\n",
    "\n",
    "        for sent in line.split('.'):\n",
    "            if contain_keys(sent, self.key_words['interest_break'], True):\n",
    "                break\n",
    "            sent = self.select_line_part(re.sub(\"\\s+\", \" \", re.sub(\"\\n\", \",\", sent)))\n",
    "            sent = self.replace_words(sent)\n",
    "            # print \"    line    :   \", sent\n",
    "            for x in re.split(\"[,:;?]\", sent):\n",
    "                if x and x.strip():\n",
    "                    tag = x.strip().lower()\n",
    "                    # print \"   tag :  \", tag\n",
    "                    if ' ' in tag and not contain_keys(tag, tags, True) and tag.count(' ') < 4:\n",
    "                        tags.append(tag)\n",
    "\n",
    "        return tags\n",
    "    \n",
    "\n",
    "    def get_personal_website(self, l, faculty_url):\n",
    "        potential_name = re.findall(r\"([A-Z]?[a-z]+)\", faculty_url) + ['personal']\n",
    "\n",
    "        potential_name = [e for e in potential_name if not contain_keys(e, self.key_words['keys'] + self.key_words[\n",
    "            'stop_word'] + self.key_words['notprof'] + ['people', self.university_name])]\n",
    "        faculty_page = ''\n",
    "        mail = ''\n",
    "\n",
    "        for a in l:\n",
    "            href = a.get('href')\n",
    "            \n",
    "            if not href or len(href) < 5:\n",
    "                continue\n",
    "                \n",
    "            suffix = href.split('.')[-1]\n",
    "            \n",
    "            if len(suffix) < 5 and contain_keys(suffix, self.key_words['skip_file']):\n",
    "                continue\n",
    "            \n",
    "            if contain_keys(href, potential_name, True) and href not in faculty_url and not onsocial(\n",
    "                    href):\n",
    "                if href.startswith('mailto:'):\n",
    "                    mail = href\n",
    "                else:\n",
    "                    faculty_page = href\n",
    "\n",
    "        return faculty_page, mail\n",
    "    \n",
    "    def find_faculty_list(self, l, faculty_url):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        links = []\n",
    "        faculty_list = []\n",
    "        # names = find_name_in_emails(l)\n",
    "        # print names\n",
    "        for e in l:\n",
    "            if self.filter_list(e):\n",
    "                continue\n",
    "\n",
    "            href = e.get('href')\n",
    "            faculty_link = format_url(href, self.domain)\n",
    "            if faculty_link.startswith(\"Error\"):\n",
    "                print \"Error!!!!!! at\", href\n",
    "                continue\n",
    "\n",
    "            if faculty_link == faculty_url:\n",
    "                continue\n",
    "\n",
    "            if faculty_link in links:\n",
    "                name = e.get_text()\n",
    "                # print name\n",
    "                i = links.index(faculty_link)\n",
    "                if name and not faculty_list[i].string:\n",
    "                    e['href'] = faculty_link\n",
    "                    faculty_list[i] = e\n",
    "                continue\n",
    "            links.append(faculty_link)\n",
    "            e['href'] = faculty_link\n",
    "            faculty_list.append(e)\n",
    "            count += 1\n",
    "\n",
    "            name = e.string\n",
    "            if name is not None and (re.search('Website', name, re.I) or\n",
    "                                     re.search('personal', name, re.I)):\n",
    "                # print \"personal website:\", faculty_link\n",
    "                pass\n",
    "            else:\n",
    "                # print \"faculty page:\", faculty_link\n",
    "                pass\n",
    "        return count, faculty_list\n",
    "    \n",
    "    def dive_into_page(self, faculty_ele):\n",
    "        faculty_link = faculty_ele.get(\"href\")\n",
    "        person = {'name': '', 'link': faculty_link, 'tags': None,\n",
    "                'position': False, 'term': ''}\n",
    "        # print(\"dive into \" + faculty_link)\n",
    "        if contain_keys(faculty_link.split('/')[-1], self.key_words['skip_file']):\n",
    "            return person\n",
    "\n",
    "        if faculty_ele.string and contain_keys(faculty_link, faculty_ele.string.split(), True):\n",
    "            person['name'] = faculty_ele.string\n",
    "\n",
    "        content, soup = self.open_page(faculty_link)\n",
    "        if content.startswith('Error to load'):\n",
    "            print \"Error!!!!!! at the link\", faculty_link\n",
    "            return person\n",
    "        \n",
    "        tags = self.get_research_interests(soup, content, [])\n",
    "        # print(\"get tags %d ge\" % len(tags))\n",
    "        \n",
    "        anchors = self.find_all_anchor(soup, self.domain, faculty_link)\n",
    "        faculty_page, mail = self.get_personal_website(anchors, faculty_link)\n",
    "\n",
    "        if faculty_page:\n",
    "            \n",
    "            # print('website page url %s ' % faculty_page)\n",
    "            faculty_page = format_url(faculty_page, self.domain, faculty_link)\n",
    "            page_c, page_soup = self.open_page(faculty_page)\n",
    "            if page_c.startswith('Error to load'):\n",
    "                print \"Error!!!!!! at the page\", faculty_page\n",
    "            else:\n",
    "                tags = self.get_research_interests(page_soup, page_c, tags)\n",
    "                # print(\"get tags %d ge\" % len(tags))\n",
    "                position, term = self.get_open_position(page_soup, page_c)\n",
    "                person['position'] = position\n",
    "                person['term'] = term\n",
    "\n",
    "        if len(tags) > 1 and tags[0].startswith(\"I'm so stupid to\"):\n",
    "            tags = tags[1:]\n",
    "        if len(tags) == 1 and tags[0].startswith(\"I'm so stupid to\"):\n",
    "            tags = []\n",
    "        \n",
    "        person['website'] = faculty_page\n",
    "        person['mail'] = mail\n",
    "        person['tags'] = tags\n",
    "\n",
    "        return person\n",
    "       \n",
    "    def get_research_interests(self, soup, content, tags):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # 先用 完整的 research interests 找\n",
    "        result = soup.find_all(string=re.compile(\"research\\s+interest\", re.I))\n",
    "        # print(\"research interest has %d \" % len(result))\n",
    "        \n",
    "        if len(result) == 1:\n",
    "            # print \"      position\", result[0].lower().find(\"interest\")\n",
    "            if len(result[0]) > result[0].lower().find(\"interest\") + 15:\n",
    "                # print \"by line\"\n",
    "                # print \"from the line\"\n",
    "                line = self.select_line_part(re.sub(\"\\s+\", \" \", re.sub(\"\\n\", \",\", result[0])))\n",
    "                research_tags = self.extract_from_line(line, tags)\n",
    "                \n",
    "                if research_tags:\n",
    "                    \n",
    "                    # print(\" extract from line %d  ge \" % len(research_tags))\n",
    "                    # print research_tags\n",
    "                    return research_tags\n",
    "            node = result[0].parent\n",
    "            tags = self.extract_from_sibling(node, tags)\n",
    "            # print(\" extract from sibling %d  ge \" % len(tags))\n",
    "            return tags\n",
    "        elif len(result) > 1:\n",
    "            for n in result:\n",
    "                if len(n) > 19:\n",
    "                    tags = self.extract_from_line(n, tags)\n",
    "                    # print(\" extract from line %d  ge \" % len(tags))\n",
    "                node = n.parent\n",
    "                tags = self.extract_from_sibling(node, tags)\n",
    "            return tags\n",
    "            \n",
    "        # 再用 research or interests to find\n",
    "        # then filter it by some rules\n",
    "        result = soup.find_all(string=re.compile(\"(research|focuses on|Expertise)\", re.I))\n",
    "        nodes = self.filter_research_interests(result)\n",
    "        #print(\"only one has %d \"%len(nodes))\n",
    "        #print tags\n",
    "\n",
    "        if len(nodes) == 1:\n",
    "            # print nodes[0]\n",
    "            if len(nodes[0]) > 19:\n",
    "                # print \"by line\",\n",
    "                research_tags = self.extract_from_line(result[0], tags)\n",
    "                # print(\" extract from line %d  ge \" % len(research_tags))\n",
    "                if research_tags:\n",
    "                    return research_tags\n",
    "            node = nodes[0].parent\n",
    "            # print node.next_sibling\n",
    "            tags = self.extract_from_sibling(node, tags)\n",
    "            # print(\" extract from sibling %d  ge \" % len(tags))\n",
    "            return tags\n",
    "        # else 搞不定了\n",
    "            \n",
    "        if not len(tags):\n",
    "            return [u\"I'm so stupid to found it,我太蠢了找不到\"]\n",
    "        return tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.cse.unt.edu/~rakl\n",
      "[u'check out', u'courses that']\n",
      "1 http://www.cse.unt.edu/~blanco/\n",
      "[u'natural language processing', u'computational semantics', u'semantic relation extraction']\n",
      "2 http://www.cse.unt.edu/~bryant\n",
      "[]\n",
      "3 http://www.cse.unt.edu/~reneebryce/\n",
      "[]\n",
      "4 http://www.cse.unt.edu/~bbuckles\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "crawler = ResearchCrawler()\n",
    "for i in range(6, 7):#len(urls)):\n",
    "    directory_url = urls[i]\n",
    "    example = examples[i]\n",
    "    # print directory_url, example\n",
    "    c, l = crawler.crawl_faculty_list(directory_url, example)\n",
    "    for e in l[:5]:\n",
    "        print l.index(e), e.get('href')\n",
    "        # print e.get_text()\n",
    "        x = crawler.dive_into_page(e)\n",
    "\n",
    "        print x['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in l[1:3]:\n",
    "    href = e.get('href')\n",
    "    content, soup = crawler.open_page(href)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://science.iit.edu/people/faculty/gady-agam\n"
     ]
    }
   ],
   "source": [
    "print href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://science.iit.edu/computer-science/people/faculty http://science.iit.edu/people/faculty/eunice-santos\n",
      "http://science.iit.edu/computer-science/people/faculty 347\n",
      "http://science.iit.edu/computer-science/people/faculty 168\n"
     ]
    }
   ],
   "source": [
    "crawler = ResearchCrawler()\n",
    "directory_url = urls[1]\n",
    "example = examples[1]\n",
    "print directory_url, example\n",
    "c, l = crawler.crawl_faculty_list(directory_url, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 [u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'science', 'iit', 'edu', 'computer', 'science', 'people', 'faculty']\n"
     ]
    }
   ],
   "source": [
    "print len(l), crawler.key_words['stop_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cs.ucf.edu/people/index.php\n",
      "http://www.cs.ucf.edu/people/index.php 651\n",
      "http://www.cs.ucf.edu/people/index.php 257 [u'http', u'www', u'cs', u'ucf', u'edu', u'bagci', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.cs.ucf.edu/~bagci/software.html\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'csdept', u'faculty', u'bassi', u'html', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'lboloni', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'crcv', u'ucf', u'edu', u'people', u'faculty', u'Borji', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'mainak', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'cse', u'eecs', u'ucf', u'edu', u'bios', u'damiandechev', u'html', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  /about.html\n",
      "[u'http', u'cal', u'ucf', u'edu', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'deo', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'dutton', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.cs.ucf.edu/~dutton/personal.html\n",
      "[u'http', u'www', u'ece', u'ucf', u'edu', u'ewetz', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.ece.ucf.edu/about/about.php\n",
      "[u'http', u'www', u'eecs', u'ucf', u'edu', u'yfallah', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.eecs.ucf.edu/%7Eyfallah/cpslab.html\n",
      "[u'http', u'www', u'eecs', u'ucf', u'edu', u'dfan', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'foroosh', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'glinos', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'crcv', u'ucf', u'edu', u'people', u'faculty', u'Gong', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'dmarino', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'guha', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.eecs.ucf.edu/~guha\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'heinrich', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.csl.cornell.edu/~heinrich/dissertation/\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'haihu', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'dsg', u'eecs', u'ucf', u'edu', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'ceh', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'jha', u'Home', u'Page', u'Home', u'Page', u'html', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'jjl', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  personal.html\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'leavens', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://academic.research.microsoft.com/Author/60393/gary-t-leavens?query=Gary%20Leavens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'xiaoman', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n",
      "website page url:  http://www.cs.ucf.edu/~xiaoman/2012PCB5596BSC4434\n",
      "[u'http', u'www', u'cs', u'ucf', u'edu', u'feiliu', 'personal'] [u'faculty', u'professor', u'lecturer', u'affiliate', u'directory/people', u'http', u'https', u'www', u'edu', u'com', u'lab', u'calendar', 'http', 'www', 'cs', 'ucf', 'edu', 'people', 'index', 'php', u'CONTACTS', u'ADMINISTRATION', u'qa', u'ADVISORY', u'BOARD', u'OPENINGS', 'people', 'ucf']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-398ae7abc016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl_from_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-dbf7d60c07f6>\u001b[0m in \u001b[0;36mcrawl_from_directory\u001b[1;34m(self, directory_url, example)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaculty_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdive_into_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-dbf7d60c07f6>\u001b[0m in \u001b[0;36mdive_into_page\u001b[1;34m(self, faculty_ele)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mperson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaculty_ele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaculty_link\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Error to load'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Error!!!!!! at the link\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfaculty_link\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-dbf7d60c07f6>\u001b[0m in \u001b[0;36mopen_page\u001b[1;34m(self, page_url)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"noframes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcontain_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"src\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"div_pass\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\software\\python27\\lib\\site-packages\\bs4\\element.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m(self, name, attrs, recursive, text, **kwargs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m         criteria.\"\"\"\n\u001b[0;32m   1277\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\software\\python27\\lib\\site-packages\\bs4\\element.pyc\u001b[0m in \u001b[0;36mfind_all\u001b[1;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m     \u001b[0mfindAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_all\u001b[0m       \u001b[1;31m# BS3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_all\u001b[0m  \u001b[1;31m# BS2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\software\\python27\\lib\\site-packages\\bs4\\element.pyc\u001b[0m in \u001b[0;36m_find_all\u001b[1;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\software\\python27\\lib\\site-packages\\bs4\\element.pyc\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[1;31m# If given a list of items, scan it for a text element that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m         \u001b[1;31m# matches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__iter__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "crawler = ResearchCrawler()\n",
    "for url in urls[8:]:\n",
    "    print url\n",
    "\n",
    "    result = crawler.crawl_from_directory(url, examples[urls.index(url)])\n",
    "    print len(result)\n",
    "    print result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pstats  \n",
    "p = pstats.Stats('my_prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.sort_stats('tottime')\n",
    "p.print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
